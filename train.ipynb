{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa19498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scripts.custom_dataset import CustomDataset\n",
    "from scripts.model import MHAModel\n",
    "from scripts.tokenizer import SeparatorTokenizer\n",
    "from scripts.vectorizer import Vectorizer\n",
    "from scripts.vocabulary import Vocabulary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95506a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROPORTION = 0.0\n",
    "EVAL_PROPORTION = 0.0\n",
    "\n",
    "ADD_BOS_EOS_TOKENS = False\n",
    "\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "\n",
    "USE_PRETRAINED = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BIAS = True\n",
    "EMBEDDING_DIM = 512\n",
    "ATTENTION_DIM = 768\n",
    "NUM_HEAD = 12\n",
    "NUM_ENCODER_LAYERS = 8\n",
    "ENCODER_FC_HIDDEN_DIM = ATTENTION_DIM*4 # Как в классическом трансформере\n",
    "CLASSIFIER_FC_HIDDEN_DIM = ATTENTION_DIM*2\n",
    "DROPOUT = 0.1\n",
    "TEMPERATURE = 1\n",
    "BATCH_FIRST = True\n",
    "\n",
    "MODEL_SAVE_FILEPATH = 'data/model_params.pt'\n",
    "DATASET_PATH = 'dataset'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bb51aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cuda.flash_sdp_enabled())\n",
    "print(torch.backends.cuda.mem_efficient_sdp_enabled())\n",
    "print(torch.backends.cuda.math_sdp_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa90c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_source_len(dataframe:pd.DataFrame)->int:\n",
    "    '''Возвращает максимальную длину входной последовательности в датафрейме'''\n",
    "    max_source_tokens = 0\n",
    "    for i in range(len(dataframe)):\n",
    "        max_source_tokens = max(len(dataframe.loc[i, 'source_tokens']), max_source_tokens)\n",
    "    return max_source_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fdcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset:CustomDataset, batch_size:int, shuffle:bool=True, drop_last:bool=True, device='cpu'):\n",
    "    '''Создает батчи из датасета и переносит данные на девайс.'''\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _ in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56f2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(model, model_filepath:str, train_states:list=None, validation_states:list=None):\n",
    "    '''Сохраняет параметры модели и метрики обучения в файлы.'''\n",
    "    torch.save(model, model_filepath)\n",
    "    if train_states is not None:\n",
    "        with open(\"data/train_states.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(train_states, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    if validation_states is not None:\n",
    "        with open(\"data/validation_states.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(validation_states, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5ae2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df:pd.DataFrame, source_column_name:str):\n",
    "    for row in range(len(df)):\n",
    "        tokens = df[source_column_name].iloc[row].copy()  # Создаем копию\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "        df.at[row, source_column_name] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4574214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(predictions:dict[str:torch.tensor], targets:dict[str:torch.tensor], target_names:list[str]):\n",
    "    for key in target_names:\n",
    "        # Для predictions: [B, S, C] -> [B*S, C]\n",
    "        if len(predictions[key].size()) == 3:\n",
    "            predictions[key] = predictions[key].contiguous().view(-1, predictions[key].size(-1))\n",
    "        \n",
    "        # Для targets: [B, S] -> [B*S]\n",
    "        if len(targets[key].size()) == 2:\n",
    "            targets[key] = targets[key].contiguous().view(-1)\n",
    "    \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions:dict[str:torch.tensor], targets:dict[str:list[int]], target_names:list[str], target_weights:dict[str:float], mask_idx:int=0):\n",
    "    predictions, targets = normalize_sizes(predictions, targets, target_names)\n",
    "    losses = {}\n",
    "    total_loss = 0\n",
    "    for key in target_names:\n",
    "        losses[key] = torch.nn.functional.cross_entropy(predictions[key], targets[key], ignore_index=mask_idx)\n",
    "        total_loss += losses[key] * target_weights[key]\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9a6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions:dict[str:torch.tensor], targets:dict[str:list[int]], target_names:list[str], mask_idx:int=0)->dict[str:float]:\n",
    "    predictions, targets = normalize_sizes(predictions, targets, target_names)\n",
    "    \n",
    "    accuracies = {}\n",
    "    for key in target_names:\n",
    "        _, pred_indices = predictions[key].max(dim=1)\n",
    "        \n",
    "        correct_indices = torch.eq(pred_indices, targets[key]).float()\n",
    "        valid_indices = torch.ne(targets[key], mask_idx).float()\n",
    "        \n",
    "        n_correct = (correct_indices * valid_indices).sum().item()\n",
    "        n_valid = valid_indices.sum().item()\n",
    "        accuracies[key] = n_correct / n_valid \n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b73d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(os.path.join(DATASET_PATH, 'ru_syntagrus-ud-train.parquet'))\n",
    "validation_df = pd.read_parquet(os.path.join(DATASET_PATH, 'ru_syntagrus-ud-dev.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88541dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SOURCE_LENGTH = max(find_max_source_len(train_df), find_max_source_len(validation_df)) + 2 # Прибавляем 2 для учета доп. токенов BOS и EOS\n",
    "target_names = ['upos']\n",
    "source_name = 'source_tokens'\n",
    "preprocess_df(train_df, source_name)\n",
    "preprocess_df(validation_df, source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d0bd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>upos</th>\n",
       "      <th>xpos</th>\n",
       "      <th>feats</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[анкета, .]</td>\n",
       "      <td>[анкета, .]</td>\n",
       "      <td>[NOUN, PUNCT]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[{'Animacy': 'Inan', 'Case': 'Nom', 'Gender': ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[root, punct]</td>\n",
       "      <td>[{'SpaceAfter': 'No'}, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[начальник, областного, управления, связи, сем...</td>\n",
       "      <td>[начальник, областной, управление, связь, Семе...</td>\n",
       "      <td>[NOUN, ADJ, NOUN, NOUN, PROPN, PROPN, AUX, NOU...</td>\n",
       "      <td>[None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>[{'Animacy': 'Anim', 'Case': 'Nom', 'Gender': ...</td>\n",
       "      <td>[8, 3, 1, 3, 1, 5, 8, 0, 8, 11, 8, 13, 11, 11,...</td>\n",
       "      <td>[nsubj, amod, nmod, nmod, appos, flat:name, co...</td>\n",
       "      <td>[None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[в, приемной, его, с, утра, ожидали, посетител...</td>\n",
       "      <td>[в, приемная, он, с, утро, ожидать, посетитель...</td>\n",
       "      <td>[ADP, NOUN, PRON, ADP, NOUN, VERB, NOUN, PUNCT...</td>\n",
       "      <td>[None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>[None, {'Animacy': 'Inan', 'Case': 'Loc', 'Gen...</td>\n",
       "      <td>[2, 6, 6, 5, 6, 0, 6, 13, 13, 13, 13, 13, 7, 1...</td>\n",
       "      <td>[case, obl, obj, case, obl, root, nsubj, punct...</td>\n",
       "      <td>[None, None, None, None, None, None, {'SpaceAf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[однако, стиль, работы, семена, еремеевича, за...</td>\n",
       "      <td>[однако, стиль, работа, Семен, Еремеевич, закл...</td>\n",
       "      <td>[ADV, NOUN, NOUN, PROPN, PROPN, VERB, ADP, PRO...</td>\n",
       "      <td>[None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>[{'Degree': 'Pos'}, {'Animacy': 'Inan', 'Case'...</td>\n",
       "      <td>[6, 6, 2, 3, 4, 0, 8, 6, 11, 11, 8, 13, 11, 16...</td>\n",
       "      <td>[advmod, nsubj, nmod, nmod, flat:name, root, c...</td>\n",
       "      <td>[None, None, None, None, None, None, None, {'S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[приемная, была, обставлена, просто, ,, но, по...</td>\n",
       "      <td>[приемная, быть, обставить, просто, ,, но, по-...</td>\n",
       "      <td>[NOUN, AUX, VERB, ADV, PUNCT, CCONJ, ADV, PUNCT]</td>\n",
       "      <td>[None, None, None, None, None, None, None, None]</td>\n",
       "      <td>[{'Animacy': 'Inan', 'Case': 'Nom', 'Gender': ...</td>\n",
       "      <td>[3, 3, 0, 3, 7, 7, 4, 3]</td>\n",
       "      <td>[nsubj:pass, aux:pass, root, advmod, punct, cc...</td>\n",
       "      <td>[None, None, None, {'SpaceAfter': 'No'}, None,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       source_tokens  \\\n",
       "0                                        [анкета, .]   \n",
       "1  [начальник, областного, управления, связи, сем...   \n",
       "2  [в, приемной, его, с, утра, ожидали, посетител...   \n",
       "3  [однако, стиль, работы, семена, еремеевича, за...   \n",
       "4  [приемная, была, обставлена, просто, ,, но, по...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0                                        [анкета, .]   \n",
       "1  [начальник, областной, управление, связь, Семе...   \n",
       "2  [в, приемная, он, с, утро, ожидать, посетитель...   \n",
       "3  [однако, стиль, работа, Семен, Еремеевич, закл...   \n",
       "4  [приемная, быть, обставить, просто, ,, но, по-...   \n",
       "\n",
       "                                                upos  \\\n",
       "0                                      [NOUN, PUNCT]   \n",
       "1  [NOUN, ADJ, NOUN, NOUN, PROPN, PROPN, AUX, NOU...   \n",
       "2  [ADP, NOUN, PRON, ADP, NOUN, VERB, NOUN, PUNCT...   \n",
       "3  [ADV, NOUN, NOUN, PROPN, PROPN, VERB, ADP, PRO...   \n",
       "4   [NOUN, AUX, VERB, ADV, PUNCT, CCONJ, ADV, PUNCT]   \n",
       "\n",
       "                                                xpos  \\\n",
       "0                                       [None, None]   \n",
       "1  [None, None, None, None, None, None, None, Non...   \n",
       "2  [None, None, None, None, None, None, None, Non...   \n",
       "3  [None, None, None, None, None, None, None, Non...   \n",
       "4   [None, None, None, None, None, None, None, None]   \n",
       "\n",
       "                                               feats  \\\n",
       "0  [{'Animacy': 'Inan', 'Case': 'Nom', 'Gender': ...   \n",
       "1  [{'Animacy': 'Anim', 'Case': 'Nom', 'Gender': ...   \n",
       "2  [None, {'Animacy': 'Inan', 'Case': 'Loc', 'Gen...   \n",
       "3  [{'Degree': 'Pos'}, {'Animacy': 'Inan', 'Case'...   \n",
       "4  [{'Animacy': 'Inan', 'Case': 'Nom', 'Gender': ...   \n",
       "\n",
       "                                                head  \\\n",
       "0                                             [0, 1]   \n",
       "1  [8, 3, 1, 3, 1, 5, 8, 0, 8, 11, 8, 13, 11, 11,...   \n",
       "2  [2, 6, 6, 5, 6, 0, 6, 13, 13, 13, 13, 13, 7, 1...   \n",
       "3  [6, 6, 2, 3, 4, 0, 8, 6, 11, 11, 8, 13, 11, 16...   \n",
       "4                           [3, 3, 0, 3, 7, 7, 4, 3]   \n",
       "\n",
       "                                              deprel  \\\n",
       "0                                      [root, punct]   \n",
       "1  [nsubj, amod, nmod, nmod, appos, flat:name, co...   \n",
       "2  [case, obl, obj, case, obl, root, nsubj, punct...   \n",
       "3  [advmod, nsubj, nmod, nmod, flat:name, root, c...   \n",
       "4  [nsubj:pass, aux:pass, root, advmod, punct, cc...   \n",
       "\n",
       "                                                misc  \n",
       "0                       [{'SpaceAfter': 'No'}, None]  \n",
       "1  [None, None, None, None, None, None, None, Non...  \n",
       "2  [None, None, None, None, None, None, {'SpaceAf...  \n",
       "3  [None, None, None, None, None, None, None, {'S...  \n",
       "4  [None, None, None, {'SpaceAfter': 'No'}, None,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ffe937",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = Vocabulary(add_bos_eos_tokens=ADD_BOS_EOS_TOKENS)\n",
    "target_vocabs = {target_name: Vocabulary(add_bos_eos_tokens=ADD_BOS_EOS_TOKENS) for target_name in target_names}\n",
    "for i in range(len(train_df)):\n",
    "    source_vocab.add_tokens(train_df[source_name].iloc[i])\n",
    "    for target_name in target_names:\n",
    "        target_vocabs[target_name].add_tokens(train_df[target_name].iloc[i])\n",
    "\n",
    "mask_index = source_vocab.mask_idx\n",
    "source_vocab_len = len(source_vocab)\n",
    "cls_names_params = {key:len(target_vocabs[key]) for key in target_names}\n",
    "target_weights = {key : 1.0 for key in target_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab0881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество батчей = 2175\n",
      "Длина словаря токенов = 121665\n",
      "Длина словаря признака upos = 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество батчей = {len(train_df)//BATCH_SIZE}')\n",
    "\n",
    "print(f'Длина словаря токенов = {len(source_vocab)}')\n",
    "for key in target_names:\n",
    "    print(f'Длина словаря признака {key} = {len(target_vocabs[key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9d64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED:\n",
    "    with open(\"data/train_states.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        train_states = json.load(file)\n",
    "\n",
    "    with open(\"data/validation_states.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        validation_states = json.load(file)\n",
    "    \n",
    "    model = torch.load(MODEL_SAVE_FILEPATH, weights_only=False)\n",
    "else:\n",
    "    train_states = []\n",
    "    validation_states = []\n",
    "    model = MHAModel(MAX_SOURCE_LENGTH, source_vocab_len, EMBEDDING_DIM, ATTENTION_DIM, NUM_HEAD, NUM_ENCODER_LAYERS, CLASSIFIER_FC_HIDDEN_DIM, ENCODER_FC_HIDDEN_DIM,\\\n",
    "                     cls_names_params, DROPOUT, TEMPERATURE, BATCH_FIRST, BIAS, mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58043435",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(source_vocab, target_vocabs, MAX_SOURCE_LENGTH, mask_index)\n",
    "dataset = CustomDataset(vectorizer, train_df, target_names, add_bos_eos_tokens=ADD_BOS_EOS_TOKENS, valid_df=validation_df)\n",
    "\n",
    "model = model.to(device=DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98501e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch 1\n",
      "Train: Суммированная ошибка эпохи 5274.725795984268\n",
      "Train: Средняя ошибка эпохи 2.425161285510011\n",
      "Train: Точность на признаке upos: 35.84785403339689\n",
      "----------\n",
      "Validation: Суммированная ошибка эпохи 645.2154250144958\n",
      "Validation: Средняя ошибка эпохи 2.3209187950161736\n",
      "Validation: Точность на признаке upos: 37.95035796643219\n",
      "----------------------------------------\n",
      "Epoch 2\n",
      "Train: Суммированная ошибка эпохи 4937.2707624435425\n",
      "Train: Средняя ошибка эпохи 2.270009545951055\n",
      "Train: Точность на признаке upos: 38.20643496385146\n",
      "----------\n",
      "Validation: Суммированная ошибка эпохи 611.0797562599182\n",
      "Validation: Средняя ошибка эпохи 2.1981286196399936\n",
      "Validation: Точность на признаке upos: 39.53194746407379\n",
      "----------------------------------------\n",
      "Epoch 3\n",
      "Train: Суммированная ошибка эпохи 4708.779181003571\n",
      "Train: Средняя ошибка эпохи 2.1649559452889924\n",
      "Train: Точность на признаке upos: 40.02086826406932\n",
      "----------\n",
      "Validation: Суммированная ошибка эпохи 584.7470057010651\n",
      "Validation: Средняя ошибка эпохи 2.1034064953275737\n",
      "Validation: Точность на признаке upos: 41.399700595678695\n",
      "----------------------------------------\n",
      "Epoch 4\n",
      "Train: Суммированная ошибка эпохи 4520.062109470367\n",
      "Train: Средняя ошибка эпохи 2.0781894756185553\n",
      "Train: Точность на признаке upos: 42.07490539696001\n",
      "----------\n",
      "Validation: Суммированная ошибка эпохи 562.1536260843277\n",
      "Validation: Средняя ошибка эпохи 2.0221353456270794\n",
      "Validation: Точность на признаке upos: 42.99493094655892\n",
      "----------------------------------------\n",
      "Epoch 5\n",
      "Train: Суммированная ошибка эпохи 4351.945664525032\n",
      "Train: Средняя ошибка эпохи 2.0008945584023166\n",
      "Train: Точность на признаке upos: 43.70186174631988\n",
      "----------\n",
      "Validation: Суммированная ошибка эпохи 541.7967113256454\n",
      "Validation: Средняя ошибка эпохи 1.9489090335454873\n",
      "Validation: Точность на признаке upos: 44.818292280664984\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        dataset.set_dataframe_split('train')\n",
    "        batch_generator = generate_batches(dataset, BATCH_SIZE, SHUFFLE, DROP_LAST, DEVICE)\n",
    "        epoch_sum_train_loss = 0.0\n",
    "        epoch_running_train_loss = 0.0\n",
    "        train_running_acc = {key:0.0 for key in target_names}\n",
    "        model.train()\n",
    "        for batch_idx, batch_dict in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions = model(batch_dict['source_x'])\n",
    "\n",
    "            total_loss, losses = compute_loss(predictions, batch_dict, target_names, target_weights, mask_index)\n",
    "\n",
    "            total_loss.backward()\n",
    "\n",
    "            scheduler.step(epoch_running_train_loss)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Средние потери и точность\n",
    "            epoch_running_train_loss += (total_loss.item() - epoch_running_train_loss) / (batch_idx + 1)\n",
    "            epoch_sum_train_loss += total_loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(predictions, batch_dict, target_names, mask_index)\n",
    "            for key in target_names:\n",
    "                train_running_acc[key] += (acc_t[key] - train_running_acc[key]) / (batch_idx + 1)\n",
    "\n",
    "        dataset.set_dataframe_split('validation')\n",
    "        batch_generator = generate_batches(dataset, BATCH_SIZE, SHUFFLE, DROP_LAST, DEVICE)\n",
    "        epoch_sum_valid_loss = 0.0\n",
    "        epoch_running_valid_loss = 0.0\n",
    "        valid_running_acc = {key:0.0 for key in target_names}\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_dict in enumerate(batch_generator):\n",
    "                \n",
    "                predictions = model(batch_dict['source_x'])\n",
    "\n",
    "                total_loss, losses = compute_loss(predictions, batch_dict, target_names, target_weights, mask_index)\n",
    "\n",
    "                # Средние потери и точность\n",
    "                epoch_running_valid_loss += (total_loss.item() - epoch_running_valid_loss) / (batch_idx + 1)\n",
    "                epoch_sum_valid_loss += total_loss.item()\n",
    "\n",
    "                acc_t = compute_accuracy(predictions, batch_dict, target_names, mask_index)\n",
    "                for key in target_names:\n",
    "                    valid_running_acc[key] += (acc_t[key] - valid_running_acc[key]) / (batch_idx + 1)\n",
    "\n",
    "        train_states.append({'epoch' : epoch+1, 'epoch_sum_train_loss' : epoch_sum_train_loss, 'epoch_running_train_loss' : epoch_running_train_loss, 'accuracy' : train_running_acc})\n",
    "        validation_states.append({'epoch' : epoch+1, 'epoch_sum_valid_loss' : epoch_sum_valid_loss, 'epoch_running_valid_loss' : epoch_running_valid_loss, 'accuracy' : valid_running_acc})\n",
    "        \n",
    "        print('-'*40)\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'Train: Суммированная ошибка эпохи {epoch_sum_train_loss}')\n",
    "        print(f'Train: Средняя ошибка эпохи {epoch_running_train_loss}')\n",
    "        for key in target_names:\n",
    "            print(f'Train: Точность на признаке {key}: {train_running_acc[key]*100}')\n",
    "\n",
    "        print('-'*10)\n",
    "        print(f'Validation: Суммированная ошибка эпохи {epoch_sum_valid_loss}')\n",
    "        print(f'Validation: Средняя ошибка эпохи {epoch_running_valid_loss}')\n",
    "        for key in target_names:\n",
    "            print(f'Validation: Точность на признаке {key}: {valid_running_acc[key]*100}')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Принудительная остановка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7641e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to_file(model, MODEL_SAVE_FILEPATH, train_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
